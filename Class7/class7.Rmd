---
title: 'Class 7: Machine Learning 1'
author: "Macy Hoang"
date: "2/8/2022"
output: pdf_document
---

# Clustering methods

Find groups (aka) clusters in my data

## K-means clustering 

Make up some data to test with. 

```{r}
# Make up some data with 2 clear groups
tmp <- c(rnorm(30, mean = 3), rnorm(30, mean= -3))
x <- cbind(tmp, rev(tmp))

plot(x)
```
The `kmeans()` function does k-means clustering
```{r}
k <- kmeans(x, centers = 4, nstart = 20)
k
```
We can use the dollar syntax to get at the results (components of the returned list)

> Q1. How many points are in each cluster?

```{r}
k$size

```

> Q2. What 'componeny' of your result object details
  - cluster size?
  - cluster assignment/membership?
  - cluster center? 

```{r}
k$size
```


```{r}
k$cluster
```

```{r}
k$centers
```

> Q3. Plot x colored by the kmeans cluster assignment and add cluster centers as blue points

```{r}
plot(x, col=k$cluster)
points(k$centers, col = "blue", pch=15, cex= 2)
```

## Hierarchial Clustering

The hclust() needs a distance matrix as input not our original data. For this we use the `dist()` function.

```{r}
hc <- hclust(dist(x))
hc
```

```{r}
plot(hc)
abline(h=10, col = "red")
```

To get our cluster membership vector, we need to cut our tree and for this we use the `cutree()`

```{r}
cutree(hc, h=10)
```

You can cut by a different height h= or into a given number of k groups with k= 

```{r}
cutree(hc, k=2)
```

# Principal Component Analysis

## PCA of UK food data

Let's read our data about the weird stuff folks from the UK eat and drink:


```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
```

> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
dim(x)
```

Look at the first bit of the file:

```{r}
## Preview the first 6 rows
head(x)
```
> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

- I prefer setting the argument with row.names because each time we use the minus indexing, it does not show all the data when you run it multiple times

Well let's set the rownames() and remove the first column

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names = 1)
```

```{r}
# Just looking at columns
ncol(x)
```

```{r}
## Complete the following code to find out how many rows and columns are in x?
dim(x)
```


We can make some plots to try to understand this data a bit more. For example barplots:

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

# PCA to the rescue

The main base R function for PCA is called `prcomp()`

> Q3. Changing what optional argument in the above barplot() function results in the following plot?
- adding the argument beside = T

```{r}
pca <- prcomp( t( x ) )
summary(pca)
```

What is this returned pca object?

```{r}
attributes(pca)
```
```{r}
plot(pca$x[,1:2], col= c("orange","red", "blue", "green"), pch=15)
text(pca$x[,1], pca$x[,2], labels = colnames(x))
```

We can look at how the variables contribute to our new PCs by examining the `pca$rotation` component of our results. 

```{r}
barplot(pca$rotation[,1], las= 2)
```

# An RNA-Seq PCA example

Read the data first

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```
> Q10. How many genes and samples are in this data

How many genes (how many rows)?

```{r}
nrow(rna.data)
```

How many experiments (columns)?

```{r}
ncol(rna.data)
```

Let's do PCA of this data set First take the transpose as that is what the `prcomp()` function wants

```{r}
pca <- prcomp(t (rna.data), scale = TRUE)
summary(pca)
```

We can make our score plot (aka PCA plot) from the `pca$x` 

```{r}
plot(pca$x[,1], pca$x[,2])
```
Make a little color vector to color up our plot by wt and ko

```{r}
colvec <- c (rep("red", 5), rep("blue", 5))
plot(pca$x[,1], pca$x[,2], col= colvec, pch= 15)
```

